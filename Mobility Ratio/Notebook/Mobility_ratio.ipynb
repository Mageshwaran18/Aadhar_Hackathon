{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invisible Citizens - Comprehensive Analysis Pipeline\n",
    "## Advanced Analytics for High-Risk Area Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets\n",
    "high_risk_pincodes = pd.read_csv('high_risk_pincodes.csv')\n",
    "state_summary = pd.read_csv('state_summary.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(\"=\"*60)\n",
    "print(\"HIGH RISK PINCODES DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Records: {len(high_risk_pincodes)}\")\n",
    "print(f\"Columns: {list(high_risk_pincodes.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(high_risk_pincodes.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATE SUMMARY DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total States: {len(state_summary)}\")\n",
    "display(state_summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': high_risk_pincodes.columns,\n",
    "    'Missing_Count': high_risk_pincodes.isnull().sum(),\n",
    "    'Missing_Percent': (high_risk_pincodes.isnull().sum() / len(high_risk_pincodes) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"Missing Data Analysis:\")\n",
    "display(missing_data[missing_data['Missing_Count'] > 0])\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData Types:\")\n",
    "display(high_risk_pincodes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary and Risk Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive risk score\n",
    "def calculate_risk_score(df):\n",
    "    \"\"\"\n",
    "    Calculate multi-dimensional risk score based on available metrics\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify numeric columns for scoring\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove ID columns if present\n",
    "    exclude_cols = ['pincode', 'id', 'index']\n",
    "    score_cols = [col for col in numeric_cols if col.lower() not in exclude_cols]\n",
    "    \n",
    "    if len(score_cols) > 0:\n",
    "        # Normalize the scores\n",
    "        scaler = StandardScaler()\n",
    "        normalized = scaler.fit_transform(df[score_cols].fillna(0))\n",
    "        \n",
    "        # Calculate composite score\n",
    "        df['composite_risk_score'] = normalized.mean(axis=1)\n",
    "        df['risk_percentile'] = df['composite_risk_score'].rank(pct=True) * 100\n",
    "        \n",
    "        # Categorize risk levels\n",
    "        df['risk_category'] = pd.cut(df['risk_percentile'], \n",
    "                                      bins=[0, 25, 50, 75, 100],\n",
    "                                      labels=['Low', 'Medium', 'High', 'Critical'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "high_risk_enriched = calculate_risk_score(high_risk_pincodes)\n",
    "\n",
    "print(\"Risk Score Distribution:\")\n",
    "if 'risk_category' in high_risk_enriched.columns:\n",
    "    display(high_risk_enriched['risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level aggregation\n",
    "if 'state' in high_risk_enriched.columns or 'State' in high_risk_enriched.columns:\n",
    "    state_col = 'state' if 'state' in high_risk_enriched.columns else 'State'\n",
    "    \n",
    "    state_analysis = high_risk_enriched.groupby(state_col).agg({\n",
    "        'pincode': 'count',\n",
    "        'composite_risk_score': ['mean', 'std', 'min', 'max']\n",
    "    }).round(3)\n",
    "    \n",
    "    state_analysis.columns = ['Total_Pincodes', 'Avg_Risk', 'StdDev_Risk', 'Min_Risk', 'Max_Risk']\n",
    "    state_analysis = state_analysis.sort_values('Avg_Risk', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 High-Risk States:\")\n",
    "    display(state_analysis.head(10))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Top states by average risk\n",
    "    state_analysis.head(15)['Avg_Risk'].plot(kind='barh', ax=axes[0], color='crimson')\n",
    "    axes[0].set_title('Top 15 States by Average Risk Score', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Average Risk Score')\n",
    "    \n",
    "    # Pincode count by state\n",
    "    state_analysis.head(15)['Total_Pincodes'].plot(kind='barh', ax=axes[1], color='steelblue')\n",
    "    axes[1].set_title('Top 15 States by Number of High-Risk Pincodes', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Pincodes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('state_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Analysis - Identifying Similar Risk Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "numeric_features = high_risk_enriched.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = ['pincode', 'id', 'index', 'composite_risk_score', 'risk_percentile']\n",
    "cluster_features = [col for col in numeric_features if col.lower() not in exclude_cols]\n",
    "\n",
    "if len(cluster_features) >= 2:\n",
    "    # Prepare clustering data\n",
    "    X_cluster = high_risk_enriched[cluster_features].fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_cluster)\n",
    "    \n",
    "    # Determine optimal clusters using elbow method\n",
    "    inertias = []\n",
    "    K_range = range(2, min(11, len(X_scaled)//10))\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    plt.ylabel('Inertia', fontsize=12)\n",
    "    plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('elbow_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply K-means with optimal k (assume 5)\n",
    "    optimal_k = 5\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    high_risk_enriched['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    print(f\"\\nCluster Distribution:\")\n",
    "    display(high_risk_enriched['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cluster_features) >= 2:\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=high_risk_enriched['cluster'], \n",
    "                         cmap='viridis', \n",
    "                         s=50, \n",
    "                         alpha=0.6,\n",
    "                         edgecolors='black',\n",
    "                         linewidth=0.5)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "    plt.title('PCA Visualization of High-Risk Areas by Cluster', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('pca_clusters.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPCA Explained Variance:\")\n",
    "    print(f\"PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "    print(f\"PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "    print(f\"Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Priority Area Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top priority areas\n",
    "if 'risk_percentile' in high_risk_enriched.columns:\n",
    "    priority_threshold = 90\n",
    "    critical_areas = high_risk_enriched[high_risk_enriched['risk_percentile'] >= priority_threshold].copy()\n",
    "    \n",
    "    print(f\"\\nCritical Priority Areas (Risk Percentile >= {priority_threshold}):\")\n",
    "    print(f\"Total: {len(critical_areas)} pincodes\")\n",
    "    \n",
    "    # Sort by risk score\n",
    "    display_cols = ['pincode', 'composite_risk_score', 'risk_percentile', 'risk_category']\n",
    "    if 'state' in critical_areas.columns:\n",
    "        display_cols.insert(1, 'state')\n",
    "    \n",
    "    available_cols = [col for col in display_cols if col in critical_areas.columns]\n",
    "    \n",
    "    print(\"\\nTop 20 Highest Risk Pincodes:\")\n",
    "    display(critical_areas.nlargest(20, 'composite_risk_score')[available_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "if len(cluster_features) >= 2:\n",
    "    correlation_matrix = high_risk_enriched[cluster_features].corr()\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=1,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final comprehensive report\n",
    "final_report = high_risk_enriched.copy()\n",
    "\n",
    "# Export to CSV\n",
    "final_report.to_csv('FINAL_COMPREHENSIVE_REPORT.csv', index=False)\n",
    "print(\"✓ Exported: FINAL_COMPREHENSIVE_REPORT.csv\")\n",
    "\n",
    "# Export critical priorities\n",
    "if 'risk_percentile' in final_report.columns:\n",
    "    critical_areas.to_csv('CRITICAL_PRIORITY_AREAS.csv', index=False)\n",
    "    print(\"✓ Exported: CRITICAL_PRIORITY_AREAS.csv\")\n",
    "\n",
    "# Export state summary\n",
    "if 'state' in high_risk_enriched.columns or 'State' in high_risk_enriched.columns:\n",
    "    state_analysis.to_csv('STATE_RISK_ANALYSIS.csv')\n",
    "    print(\"✓ Exported: STATE_RISK_ANALYSIS.csv\")\n",
    "\n",
    "# Export cluster profiles\n",
    "if 'cluster' in final_report.columns:\n",
    "    cluster_profiles = final_report.groupby('cluster').agg({\n",
    "        'pincode': 'count',\n",
    "        'composite_risk_score': ['mean', 'std'],\n",
    "        **{col: 'mean' for col in cluster_features}\n",
    "    }).round(3)\n",
    "    cluster_profiles.to_csv('CLUSTER_PROFILES.csv')\n",
    "    print(\"✓ Exported: CLUSTER_PROFILES.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - ALL REPORTS GENERATED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"# EXECUTIVE SUMMARY\")\n",
    "print(\"#\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"Total High-Risk Pincodes Analyzed: {len(high_risk_enriched):,}\")\n",
    "\n",
    "if 'risk_category' in high_risk_enriched.columns:\n",
    "    print(f\"\\nRisk Distribution:\")\n",
    "    for cat in ['Critical', 'High', 'Medium', 'Low']:\n",
    "        count = (high_risk_enriched['risk_category'] == cat).sum()\n",
    "        pct = count / len(high_risk_enriched) * 100\n",
    "        print(f\"  {cat}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if 'cluster' in high_risk_enriched.columns:\n",
    "    print(f\"\\nIdentified {optimal_k} distinct risk profile clusters\")\n",
    "\n",
    "if 'state' in high_risk_enriched.columns or 'State' in high_risk_enriched.columns:\n",
    "    state_col = 'state' if 'state' in high_risk_enriched.columns else 'State'\n",
    "    top_state = state_analysis.index[0]\n",
    "    print(f\"\\nHighest Risk State: {top_state}\")\n",
    "    print(f\"  Average Risk Score: {state_analysis.iloc[0]['Avg_Risk']:.3f}\")\n",
    "    print(f\"  Number of Pincodes: {int(state_analysis.iloc[0]['Total_Pincodes']):,}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"# RECOMMENDED ACTIONS\")\n",
    "print(\"#\"*60 + \"\\n\")\n",
    "print(\"1. Prioritize resources for Critical and High-risk categories\")\n",
    "print(\"2. Deploy targeted interventions based on cluster profiles\")\n",
    "print(\"3. Focus immediate attention on top 10% risk percentile areas\")\n",
    "print(\"4. Conduct detailed field assessments in identified hotspots\")\n",
    "print(\"5. Monitor trends in state-level risk variations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

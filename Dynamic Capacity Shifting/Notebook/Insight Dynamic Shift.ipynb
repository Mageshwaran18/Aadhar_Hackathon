{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini - Comprehensive Data Analysis Pipeline\n",
    "## Advanced Analytics, Visualization & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (adjust filename as needed)\n",
    "# Replace 'your_data.csv' with your actual filename\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Records: {len(df):,}\")\n",
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLUMN INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col:30s} - {df[col].dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIRST 10 ROWS\")\n",
    "print(\"=\"*70)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "\n",
    "missing_data = missing_data.sort_values('Missing_Count', ascending=False)\n",
    "display(missing_data)\n",
    "\n",
    "# Visualize missing data\n",
    "if missing_data['Missing_Count'].sum() > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missing_cols = missing_data[missing_data['Missing_Count'] > 0]\n",
    "    plt.barh(missing_cols['Column'], missing_cols['Missing_Percentage'], color='coral')\n",
    "    plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
    "    plt.title('Missing Data by Column', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('01_missing_data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 01_missing_data_analysis.png\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Statistical Summary & Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COLUMN TYPES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNumeric Columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"\\nCategorical Columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "\n",
    "# Statistical summary for numeric columns\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL SUMMARY - NUMERIC COLUMNS\")\n",
    "    print(\"=\"*70)\n",
    "    display(df[numeric_cols].describe().round(3))\n",
    "\n",
    "# Frequency analysis for categorical columns\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATEGORICAL COLUMNS - UNIQUE VALUES\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"{col:30s} - {unique_count:5d} unique values\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"  Values: {df[col].value_counts().to_dict()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Distribution Visualization for Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 0:\n",
    "    # Select key numeric columns for visualization (limit to 12)\n",
    "    vis_cols = numeric_cols[:12]\n",
    "    \n",
    "    # Create distribution plots\n",
    "    n_cols = min(4, len(vis_cols))\n",
    "    n_rows = (len(vis_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))\n",
    "    axes = axes.flatten() if len(vis_cols) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(vis_cols):\n",
    "        if idx < len(axes):\n",
    "            # Remove NaN values for plotting\n",
    "            data = df[col].dropna()\n",
    "            \n",
    "            axes[idx].hist(data, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "            axes[idx].axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.2f}')\n",
    "            axes[idx].axvline(data.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {data.median():.2f}')\n",
    "            axes[idx].set_title(f'{col}\\n(n={len(data):,})', fontweight='bold')\n",
    "            axes[idx].set_xlabel('Value')\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "            axes[idx].legend()\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(vis_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Distribution of Numeric Features', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('02_numeric_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 02_numeric_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) >= 2:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix), k=1)\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                linewidths=1,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                vmin=-1, vmax=1)\n",
    "    \n",
    "    plt.title('Correlation Matrix - Numeric Features', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('03_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 03_correlation_matrix.png\")\n",
    "    \n",
    "    # Find strong correlations\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STRONG CORRELATIONS (|r| > 0.7)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                strong_corr.append({\n",
    "                    'Feature_1': correlation_matrix.columns[i],\n",
    "                    'Feature_2': correlation_matrix.columns[j],\n",
    "                    'Correlation': correlation_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if strong_corr:\n",
    "        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlation', key=abs, ascending=False)\n",
    "        display(strong_corr_df)\n",
    "    else:\n",
    "        print(\"No strong correlations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Outlier Detection & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 0:\n",
    "    # Create boxplots for outlier detection\n",
    "    vis_cols = numeric_cols[:12]\n",
    "    n_cols = min(4, len(vis_cols))\n",
    "    n_rows = (len(vis_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))\n",
    "    axes = axes.flatten() if len(vis_cols) > 1 else [axes]\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    for idx, col in enumerate(vis_cols):\n",
    "        if idx < len(axes):\n",
    "            data = df[col].dropna()\n",
    "            \n",
    "            # Calculate outliers using IQR method\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "            outlier_pct = len(outliers) / len(data) * 100\n",
    "            \n",
    "            outlier_summary.append({\n",
    "                'Feature': col,\n",
    "                'Outlier_Count': len(outliers),\n",
    "                'Outlier_Percentage': outlier_pct\n",
    "            })\n",
    "            \n",
    "            # Create boxplot\n",
    "            axes[idx].boxplot(data, vert=True)\n",
    "            axes[idx].set_title(f'{col}\\nOutliers: {len(outliers)} ({outlier_pct:.1f}%)', fontweight='bold')\n",
    "            axes[idx].set_ylabel('Value')\n",
    "            axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(vis_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Outlier Detection - Boxplots', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('04_outlier_detection.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 04_outlier_detection.png\")\n",
    "    \n",
    "    # Display outlier summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"OUTLIER SUMMARY\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier_Percentage', ascending=False)\n",
    "    display(outlier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Clustering Analysis (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) >= 2:\n",
    "    # Prepare data for clustering\n",
    "    X = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Elbow method to find optimal k\n",
    "    inertias = []\n",
    "    K_range = range(2, min(11, len(df)//10))\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
    "    plt.ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "    plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('05_elbow_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 05_elbow_curve.png\")\n",
    "    \n",
    "    # Apply K-means with optimal k (default: 5)\n",
    "    optimal_k = 5\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    print(f\"\\nâœ“ Created {optimal_k} clusters\")\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    display(df['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) >= 2 and 'Cluster' in df.columns:\n",
    "    # Apply PCA for 2D visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create PCA plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=df['Cluster'], \n",
    "                         cmap='viridis', \n",
    "                         s=50, \n",
    "                         alpha=0.6,\n",
    "                         edgecolors='black',\n",
    "                         linewidth=0.5)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "    plt.title('PCA Visualization - Cluster Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('06_pca_clusters.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 06_pca_clusters.png\")\n",
    "    \n",
    "    print(f\"\\nPCA Explained Variance:\")\n",
    "    print(f\"  PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "    print(f\"  PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "    print(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Cluster' in df.columns and len(numeric_cols) > 0:\n",
    "    # Calculate cluster profiles\n",
    "    cluster_profiles = df.groupby('Cluster')[numeric_cols].mean().round(3)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"CLUSTER PROFILES - MEAN VALUES\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    display(cluster_profiles.T)  # Transpose for better readability\n",
    "    \n",
    "    # Visualize cluster profiles\n",
    "    top_features = numeric_cols[:8]  # Limit to 8 features for clarity\n",
    "    cluster_profiles_subset = df.groupby('Cluster')[top_features].mean()\n",
    "    \n",
    "    # Normalize for heatmap\n",
    "    cluster_profiles_norm = (cluster_profiles_subset - cluster_profiles_subset.min()) / (cluster_profiles_subset.max() - cluster_profiles_subset.min())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_profiles_norm.T, \n",
    "                annot=cluster_profiles_subset.T.round(2), \n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Normalized Value'},\n",
    "                linewidths=1,\n",
    "                square=False)\n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title('Cluster Profiles Heatmap (Top Features)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('07_cluster_profiles.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 07_cluster_profiles.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”Ÿ Categorical Analysis (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categorical_cols) > 0:\n",
    "    # Analyze top categorical columns\n",
    "    vis_cats = [col for col in categorical_cols if df[col].nunique() <= 20][:6]\n",
    "    \n",
    "    if len(vis_cats) > 0:\n",
    "        n_cols = min(3, len(vis_cats))\n",
    "        n_rows = (len(vis_cats) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "        axes = axes.flatten() if len(vis_cats) > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(vis_cats):\n",
    "            if idx < len(axes):\n",
    "                value_counts = df[col].value_counts().head(15)\n",
    "                \n",
    "                axes[idx].barh(range(len(value_counts)), value_counts.values, color='teal')\n",
    "                axes[idx].set_yticks(range(len(value_counts)))\n",
    "                axes[idx].set_yticklabels(value_counts.index)\n",
    "                axes[idx].set_xlabel('Count', fontsize=11)\n",
    "                axes[idx].set_title(f'{col}\\n(Top 15 values)', fontweight='bold', fontsize=12)\n",
    "                axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "                \n",
    "                # Add value labels\n",
    "                for i, v in enumerate(value_counts.values):\n",
    "                    axes[idx].text(v + max(value_counts.values)*0.01, i, f'{v:,}', \n",
    "                                 va='center', fontsize=9)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(vis_cats), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Categorical Features - Value Distribution', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('08_categorical_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\\nâœ“ Saved: 08_categorical_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Cluster' in df.columns and len(numeric_cols) >= 2:\n",
    "    # Train Random Forest to find feature importance\n",
    "    X_importance = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    y_importance = df['Cluster']\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_importance, y_importance)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    display(importance_df.head(15))\n",
    "    \n",
    "    # Visualize top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = min(15, len(importance_df))\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    plt.barh(range(len(top_features)), top_features['Importance'].values, color='darkgreen')\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'].values)\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Most Important Features', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('09_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ“ Saved: 09_feature_importance.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Export Final Results & Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export enhanced dataset with clusters\n",
    "output_file = 'gemini_analysis_complete.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"âœ“ Exported: {output_file}\")\n",
    "print(f\"  - Total rows: {len(df):,}\")\n",
    "print(f\"  - Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Export cluster summary\n",
    "if 'Cluster' in df.columns and len(numeric_cols) > 0:\n",
    "    cluster_summary = df.groupby('Cluster').agg({\n",
    "        **{col: ['mean', 'std', 'min', 'max'] for col in numeric_cols[:5]}\n",
    "    }).round(3)\n",
    "    cluster_summary.to_csv('gemini_cluster_summary.csv')\n",
    "    print(f\"\\nâœ“ Exported: gemini_cluster_summary.csv\")\n",
    "\n",
    "# Export statistical summary\n",
    "summary_stats = df[numeric_cols].describe().T\n",
    "summary_stats.to_csv('gemini_statistical_summary.csv')\n",
    "print(f\"âœ“ Exported: gemini_statistical_summary.csv\")\n",
    "\n",
    "# Export correlation matrix\n",
    "if len(numeric_cols) >= 2:\n",
    "    correlation_matrix.to_csv('gemini_correlation_matrix.csv')\n",
    "    print(f\"âœ“ Exported: gemini_correlation_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£3ï¸âƒ£ Executive Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# GEMINI - COMPREHENSIVE ANALYSIS REPORT\")\n",
    "print(\"#\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total Records: {len(df):,}\")\n",
    "print(f\"Total Features: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA COMPOSITION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNumeric Features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical Features: {len(categorical_cols)}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    for col in numeric_cols[:5]:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {df[col].mean():.3f}\")\n",
    "        print(f\"  Median: {df[col].median():.3f}\")\n",
    "        print(f\"  Std Dev: {df[col].std():.3f}\")\n",
    "        print(f\"  Range: [{df[col].min():.3f}, {df[col].max():.3f}]\")\n",
    "\n",
    "if 'Cluster' in df.columns:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLUSTERING ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nNumber of Clusters: {df['Cluster'].nunique()}\")\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    for cluster, count in df['Cluster'].value_counts().sort_index().items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  Cluster {cluster}: {count:,} records ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATED OUTPUTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCSV Files:\")\n",
    "print(\"  âœ“ gemini_analysis_complete.csv\")\n",
    "print(\"  âœ“ gemini_cluster_summary.csv\")\n",
    "print(\"  âœ“ gemini_statistical_summary.csv\")\n",
    "print(\"  âœ“ gemini_correlation_matrix.csv\")\n",
    "\n",
    "print(\"\\nVisualizations:\")\n",
    "print(\"  âœ“ 01_missing_data_analysis.png\")\n",
    "print(\"  âœ“ 02_numeric_distributions.png\")\n",
    "print(\"  âœ“ 03_correlation_matrix.png\")\n",
    "print(\"  âœ“ 04_outlier_detection.png\")\n",
    "print(\"  âœ“ 05_elbow_curve.png\")\n",
    "print(\"  âœ“ 06_pca_clusters.png\")\n",
    "print(\"  âœ“ 07_cluster_profiles.png\")\n",
    "print(\"  âœ“ 08_categorical_analysis.png\")\n",
    "print(\"  âœ“ 09_feature_importance.png\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# ANALYSIS COMPLETE - ALL REPORTS GENERATED\")\n",
    "print(\"#\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}